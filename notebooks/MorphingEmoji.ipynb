{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6c646-8608-4d2b-ad90-8ea4b3745f4c",
   "metadata": {},
   "source": [
    "# Morphing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e435ff-e7ec-47f2-9476-679e5699c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllable_nca.experiments.morphing_image.trainer import MorphingImageNCATrainer\n",
    "from controllable_nca.experiments.morphing_image.emoji_dataset import EmojiDataset\n",
    "from controllable_nca.nca import ControllableNCA\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c54c0-ac1d-4641-b9d9-ba1d36586434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "from controllable_nca.dataset import NCADataset\n",
    "from controllable_nca.utils import load_emoji, rgb\n",
    "\n",
    "\n",
    "class EmojiDataset(NCADataset):\n",
    "    # EMOJI = 'ðŸ¦ŽðŸ˜€ðŸ’¥'\n",
    "    EMOJI = \"ðŸ¦ŽðŸ˜€ðŸ’¥ðŸ‘ðŸ ðŸ¦‹ðŸžðŸ•¸ðŸ¥¨ðŸŽ„\"\n",
    "    # EMOJI = \"ðŸ¦ŽðŸ˜€ðŸ‘ðŸ•¸ðŸ¥¨ðŸŽ„\"\n",
    "\n",
    "    digits = [\n",
    "        \"0030\",  # 0\n",
    "        \"0031\",  # 1\n",
    "        \"0032\",  # 2\n",
    "        \"0033\",  # 3\n",
    "        \"0034\",  # 4\n",
    "        \"0035\",  # 5\n",
    "        \"0036\",  # 6\n",
    "        \"0037\",  # 7\n",
    "        \"0038\",  # 8\n",
    "        \"0039\",  # 9\n",
    "    ]\n",
    "\n",
    "    def __init__(self, image_size=64, thumbnail_size=32, use_one_hot: bool = False):\n",
    "        emojis = torch.stack(\n",
    "            [load_emoji(e, image_size, thumbnail_size) for e in EmojiDataset.EMOJI],\n",
    "            dim=0,\n",
    "        )\n",
    "        self.emojis = emojis\n",
    "        self.num_samples = len(self)\n",
    "        self._target_size = self.emojis.size()[-3:]\n",
    "\n",
    "    def num_goals(self):\n",
    "        return self.emojis.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            return self.emojis[idx : idx + 1].clone(), idx\n",
    "        return self.emojis[idx].clone(), idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.emojis.size(0)\n",
    "\n",
    "    def target_size(self):\n",
    "        if self._target_size is not None:\n",
    "            return self._target_size\n",
    "        self._target_size = self.emojis.size()[-3:]\n",
    "        return self._target_size\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.emojis = self.emojis.to(device)\n",
    "\n",
    "    def visualize(self, idx=0):\n",
    "        self.plot_img(self.emojis[idx : idx + 1])\n",
    "\n",
    "    def plot_img(self, img):\n",
    "        with torch.no_grad():\n",
    "            rgb_image = rgb(img, False).squeeze().detach().cpu().numpy()\n",
    "        rgb_image = rearrange(rgb_image, \"c w h -> w h c\")\n",
    "        _ = plt.imshow(rgb_image)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588342d-77bb-4c2d-939c-9c9f6c127eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmojiDataset(image_size=64, thumbnail_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4e59a-658a-4781-9bf4-ce7f485eed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.visualize(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60427a6-e940-4c95-9710-06c015887f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.visualize(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeedb4-b929-465e-8713-9577c7453c8b",
   "metadata": {},
   "source": [
    "### Make NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbd2e5-08b9-4e56-bc61-25c05dfc6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc3d69-e141-4563-95e2-887638b6cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple  # noqa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "\n",
    "from controllable_nca.utils import build_conv2d_net\n",
    "\n",
    "class DeepEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = Embedding(num_embeddings, 32)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, out_channels, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, indices):\n",
    "        embeddings = self.encoder(self.embedding(indices))\n",
    "        return embeddings\n",
    "\n",
    "NUM_HIDDEN_CHANNELS = 16\n",
    "\n",
    "encoder = DeepEncoder(dataset.num_goals(), NUM_HIDDEN_CHANNELS)\n",
    "\n",
    "nca =  ControllableNCA(num_goals=dataset.num_goals(), use_image_encoder=False, encoder=encoder, target_shape=dataset.target_size(), living_channel_dim=3, num_hidden_channels=NUM_HIDDEN_CHANNELS, cell_fire_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e0a9e-75ee-4f1a-9c82-7119e65ab9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "nca = nca.to(device)\n",
    "dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a7022-06c9-4c59-bd6c-219ea60be756",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MorphingImageNCATrainer(nca, dataset, nca_steps=[48, 96], lr=1e-3, num_damaged=0, damage_radius=3, device=device, pool_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97078f4f-c9ad-4e75-82eb-832a750a2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191755f4-3b65-41b6-83af-edebc14ece1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(batch_size=8, epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f62fa2-fea2-42fb-9629-073a3ab05383",
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.load(\"../saved_models/100k_linear_encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca987e9-f8f5-40c0-b936-4b7b5f61a0a4",
   "metadata": {},
   "source": [
    "### Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca43ee4-18b2-42d1-9d5e-7f1948ba4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Event, Thread\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from ipycanvas import Canvas, hold_canvas  # noqa\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "from controllable_nca.utils import create_2d_circular_mask, rgb\n",
    "\n",
    "\n",
    "def to_numpy_rgb(x, use_rgb=False):\n",
    "    return rearrange(\n",
    "        np.squeeze(rgb(x, use_rgb).detach().cpu().numpy()), \"c x y -> x y c\"\n",
    "    )\n",
    "\n",
    "\n",
    "class MorphingImageVisualizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainer,\n",
    "        image_size,\n",
    "        rgb: bool = False,\n",
    "        canvas_scale=5,\n",
    "        damage_radius: int = 5,\n",
    "    ):\n",
    "        self.trainer = trainer\n",
    "        self.current_state = None\n",
    "        self.current_goal = None\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.rgb = rgb\n",
    "        self.canvas_scale = canvas_scale\n",
    "        self.canvas_size = self.image_size * self.canvas_scale\n",
    "\n",
    "        self.canvas = Canvas(width=self.canvas_size, height=self.canvas_size)\n",
    "        self.canvas.on_mouse_down(self.handle_mouse_down)\n",
    "        self.stopped = Event()\n",
    "\n",
    "        self.current_goal = torch.tensor(0, device=self.trainer.device)\n",
    "\n",
    "        self.device = self.trainer.device\n",
    "        self.damage_radius = damage_radius\n",
    "        self.current_state = self.trainer.nca.generate_seed(1).to(self.device)\n",
    "\n",
    "        def button_fn(class_num):\n",
    "            def start(btn):\n",
    "                self.current_goal = torch.tensor(class_num, device=self.trainer.device)\n",
    "                if self.stopped.isSet():\n",
    "                    self.stopped.clear()\n",
    "                    Thread(target=self.loop).start()\n",
    "\n",
    "            return start\n",
    "\n",
    "        button_list = []\n",
    "        for i in range(len(self.trainer.target_dataset.EMOJI)):\n",
    "            button_list.append(Button(description=self.trainer.target_dataset.EMOJI[i]))\n",
    "            button_list[-1].on_click(button_fn(i))\n",
    "\n",
    "        self.vbox = VBox(button_list)\n",
    "\n",
    "        self.stop_btn = Button(description=\"Stop\")\n",
    "\n",
    "        def stop(btn):\n",
    "            if not self.stopped.isSet():\n",
    "                self.stopped.set()\n",
    "\n",
    "        self.stop_btn.on_click(stop)\n",
    "\n",
    "    def handle_mouse_down(self, xpos, ypos):\n",
    "        in_x = int(xpos / self.canvas_scale)\n",
    "        in_y = int(ypos / self.canvas_scale)\n",
    "\n",
    "        mask = create_2d_circular_mask(\n",
    "            self.image_size,\n",
    "            self.image_size,\n",
    "            (in_x, in_y),\n",
    "            radius=self.damage_radius,\n",
    "        )\n",
    "        self.current_state[0][:, mask] *= 0.0\n",
    "\n",
    "    def draw_image(self, rgb):\n",
    "        with hold_canvas(self.canvas):\n",
    "            rgb = np.squeeze(rearrange(rgb, \"b c w h -> b w h c\"))\n",
    "            self.canvas.clear()  # Clear the old animation step\n",
    "            self.canvas.put_image_data(\n",
    "                cv2.resize(\n",
    "                    rgb * 255.0,\n",
    "                    (self.canvas_size, self.canvas_size),\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                ),\n",
    "                0,\n",
    "                0,\n",
    "            )\n",
    "\n",
    "    def loop(self):\n",
    "        with torch.no_grad():\n",
    "            self.current_state = self.trainer.nca.generate_seed(1).to(self.device)\n",
    "            while not self.stopped.wait(0.02):  # the first call is in `interval` secs\n",
    "                # update_particle_locations()\n",
    "                self.draw_image(self.trainer.to_rgb(self.current_state))\n",
    "                self.current_state = self.trainer.nca.grow(\n",
    "                    self.current_state, 1, self.current_goal\n",
    "                )\n",
    "\n",
    "    def visualize(self):\n",
    "        Thread(target=self.loop).start()\n",
    "        display(self.canvas, HBox([self.stop_btn, self.vbox]))  # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fb42a-c2c5-4259-be16-844b7220a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = MorphingImageVisualizer(trainer, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d9cf-a084-40fc-a675-5975d52963ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pycanvas] *",
   "language": "python",
   "name": "conda-env-pycanvas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
